{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so you will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "We've already provided the code needed to load the data, perform iterative parsing and write the\n",
    "output to csv files. Your task is to complete the shape_element function that will transform each\n",
    "element into the correct format. To make this process easier we've already defined a schema (see\n",
    "the schema.py file in the last code tab) for the .csv files and the eventual tables. Using the \n",
    "cerberus library we can validate the output against this schema to ensure it is correct.\n",
    "\n",
    "## Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon\n",
    "        is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "The final return value for a \"node\" element should look something like:\n",
    "\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "          'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "          'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within\n",
    "            the way element\n",
    "\n",
    "The final return value for a \"way\" element should look something like:\n",
    "\n",
    "{'way': {'id': 209809850,\n",
    "         'user': 'chicago-buildings',\n",
    "         'uid': 674454,\n",
    "         'version': '1',\n",
    "         'timestamp': '2013-03-13T15:58:04Z',\n",
    "         'changeset': 15353317},\n",
    " 'way_nodes': [{'id': 209809850, 'node_id': 2199822281, 'position': 0},\n",
    "               {'id': 209809850, 'node_id': 2199822390, 'position': 1},\n",
    "               {'id': 209809850, 'node_id': 2199822392, 'position': 2},\n",
    "               {'id': 209809850, 'node_id': 2199822369, 'position': 3},\n",
    "               {'id': 209809850, 'node_id': 2199822370, 'position': 4},\n",
    "               {'id': 209809850, 'node_id': 2199822284, 'position': 5},\n",
    "               {'id': 209809850, 'node_id': 2199822281, 'position': 6}],\n",
    " 'way_tags': [{'id': 209809850,\n",
    "               'key': 'housenumber',\n",
    "               'type': 'addr',\n",
    "               'value': '1412'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street',\n",
    "               'type': 'addr',\n",
    "               'value': 'West Lexington St.'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:name',\n",
    "               'type': 'addr',\n",
    "               'value': 'Lexington'},\n",
    "              {'id': '209809850',\n",
    "               'key': 'street:prefix',\n",
    "               'type': 'addr',\n",
    "               'value': 'West'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:type',\n",
    "               'type': 'addr',\n",
    "               'value': 'Street'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building',\n",
    "               'type': 'regular',\n",
    "               'value': 'yes'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'levels',\n",
    "               'type': 'building',\n",
    "               'value': '1'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building_id',\n",
    "               'type': 'chicago',\n",
    "               'value': '366409'}]}\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import OrderedDict\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"example.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'user', 'uid', 'version','lat','lon', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        for a in element.attrib:\n",
    "            if a in NODE_FIELDS:\n",
    "                if a not in node_attribs:\n",
    "\n",
    "                    node_attribs[a]=element.attrib[a]\n",
    "        \n",
    "#   <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "#   should be turned into\n",
    "#   {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}       \n",
    "\n",
    "        for tag in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tag.attrib['k']):\n",
    "                \n",
    "                    \n",
    "                each_tag={}\n",
    "                each_tag['id']=node_attribs['id']\n",
    "                each_tag['value']=tag.attrib['v']\n",
    "                if LOWER_COLON.search(tag.attrib['k']):\n",
    "                    \n",
    "                    sub_attrib=tag.attrib['k'].split(':',1)\n",
    "                        \n",
    "                    each_tag['key']=sub_attrib[1]\n",
    "\n",
    "                    each_tag['type']=sub_attrib[0]\n",
    "                else:\n",
    "                    each_tag['key']=tag.attrib['k']\n",
    "                    each_tag['type']='regular'\n",
    "                    \n",
    "        \n",
    "                key_order= ('id', 'key', 'value', 'type')\n",
    "                OrderedDict((k, each_tag[k]) for k in key_order)\n",
    "                tags.append(each_tag)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        for a in element.attrib:\n",
    "            if a in WAY_FIELDS:\n",
    "                if a not in way_attribs:\n",
    "\n",
    "                    way_attribs[a]=element.attrib[a]\n",
    "        count=0\n",
    "        for nd in element.iter('nd'):\n",
    "            \n",
    "            each_way_node={}\n",
    "            \n",
    "            \n",
    "            each_way_node['position']=count\n",
    "            count+=1\n",
    "            each_way_node['node_id']=nd.attrib['ref']\n",
    "            each_way_node['id']=way_attribs['id']\n",
    "            way_nodes.append(each_way_node)\n",
    "\n",
    "        \n",
    "        for tag in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tag.attrib['k']):\n",
    "                \n",
    "                    \n",
    "                each_tag={}\n",
    "                each_tag['id']=way_attribs['id']\n",
    "                each_tag['value']=tag.attrib['v']\n",
    "                if LOWER_COLON.search(tag.attrib['k']):\n",
    "                    \n",
    "                    sub_attrib=tag.attrib['k'].split(':',1)\n",
    "                        \n",
    "                    each_tag['key']=sub_attrib[1]\n",
    "\n",
    "                    each_tag['type']=sub_attrib[0]\n",
    "                else:\n",
    "                    each_tag['key']=tag.attrib['k']\n",
    "                    each_tag['type']='regular'\n",
    "                    \n",
    "        \n",
    "                tags.append(each_tag)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas.io.data as web\n",
    "from datetime import date, datetime, timedelta\n",
    "from collections import defaultdict\n",
    "start = datetime(2010, 1, 1)\n",
    "end = date.today()\n",
    "\n",
    "c = web.DataReader('aapl', 'yahoo', start, end)\n",
    "print type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_df=pd.DataFrame(index=[0])\n",
    "out_df['eight_new_price']=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eight_new_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eight_new_price\n",
       "0            True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Complex:\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "        out_df=pd.DataFrame(index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(self):\n",
    "    \n",
    "    out_df[eight_new_price]=True\n",
    "    return out_df\n",
    "#     dragonfly_doji(data_pt)\n",
    "#     gravestone_doji(data_pt)\n",
    "#     long_legged_doji(data_pt)\n",
    "#     body_candle(data_pt)\n",
    "#     black_candle(data_pt)\n",
    "#     tall_black_candle(data_pt)\n",
    "#     small_black_candle(data_pt)\n",
    "#     white_candle(data_pt)\n",
    "#     tall_white_candle(data_pt)\n",
    "#     small_white_candle(data_pt)\n",
    "#     white_marubozu_candle(data_pt)\n",
    "#     black_marubozu_candle(data_pt)\n",
    "#     closing_black_marubozu_candle(data_pt)\n",
    "#     closing_white_marubozu_candle(data_pt)\n",
    "#     black_spinning_top_candle(data_pt)\n",
    "#     black_spinning_top_candle(data_pt)\n",
    "#    up_price_trend(data_pt, data_pt1, data_pt2)\n",
    "#    down_price_trend(data_pt, data_pt1, data_pt2)\n",
    "#    similar_price(data_pt1,data_pt2, percent = 0.001)\n",
    "\n",
    "eight_new_price(data)\n",
    "ten_new_price(data)\n",
    "twelve_new_price(data)\n",
    "thirteen_new_price(data)\n",
    "bearish_abandoned_baby(data)\n",
    "bullish_abandoned_baby(data)\n",
    "above_stomach(data)\n",
    "advance_block(data)\n",
    "below_stomach(data)\n",
    "bearish_belt_hold(data)\n",
    "bearish_breakaway(data)\n",
    "bearish_doji_star(data)\n",
    "bearish_engulfing(data)\n",
    "bearish_harami(data)\n",
    "bearish_harami_cross(data)\n",
    "bearish_kicking(data)\n",
    "bearish_meeting_lines(data)\n",
    "bearish_separating_lines(data)\n",
    "bearish_side_by_side_white_lines(data)\n",
    "bearish_three_line_strike(data)\n",
    "bearish_tri_star(data)\n",
    "bullish_belt_hold(data)\n",
    "bullish_breakaway(data)\n",
    "bullish_doji_star(data)\n",
    "bullish_engulfing(data)\n",
    "bullish_harami(data)\n",
    "bullish_harami_cross(data)\n",
    "bullish_kicking(data)\n",
    "bullish_meeting_lines(data)\n",
    "bullish_separating_lines(data)\n",
    "bullish_side_by_side_white_lines(data)\n",
    "bullish_three_line_strike(data)\n",
    "bullish_tri_star(data)\n",
    "collapsing_doji_star(data)\n",
    "conceling_baby_swallow(data)\n",
    "dark_cloud_cover(data)\n",
    "deliberation(data)\n",
    "gapping_down_doji(data)\n",
    "gapping_up_doji(data)\n",
    "northern_doji(data)\n",
    "southern_doji(data)\n",
    "bearish_doji_star(data)\n",
    "bullish_doji_star(data)\n",
    "evening_doji(data)\n",
    "downside_gap_three_methods(data)\n",
    "downside_tasuki_gap(data)\n",
    "falling_three_methods(data)\n",
    "falling_window(data)\n",
    "hammer(data)\n",
    "inverted_hammer(data)\n",
    "hanging_man(data)\n",
    "high_wave(data)\n",
    "homing_pigeon(data)\n",
    "identical_three_crows(data)\n",
    "in_neck(data)\n",
    "ladder_bottom(data)\n",
    "last_engulfing_bottom(data)\n",
    "last_engulfing_top(data)\n",
    "matching_low(data)\n",
    "mat_hold(data)\n",
    "morning_doji_star(data)\n",
    "morning_star(data)\n",
    "on_neck(data)\n",
    "piercing_pattern(data)\n",
    "rickshaw_man(data)\n",
    "rising_three_methods(data)\n",
    "rising_window(data)\n",
    "shooting_star_1(data)\n",
    "shooting_star_2(data)\n",
    "stick_sandwich(data)\n",
    "takuri_line(data)\n",
    "three_black_crows(data)\n",
    "three_inside_down(data)\n",
    "three_inside_up(data)\n",
    "three_outside_down(data)\n",
    "three_outside_up(data)\n",
    "three_stars_in_south(data)\n",
    "three_white_soldiers(data)\n",
    "thrusting(data)\n",
    "tweezers_bottom(data)\n",
    "tweezers_top(data)\n",
    "two_black_gapping(data)\n",
    "two_crows(data)\n",
    "unique_three_river_bottom(data)\n",
    "upside_gap_three_methods(data)\n",
    "upside_gap_two_crows(data)\n",
    "upside_tasuki_gap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "import pandas.io.data as web\n",
    "from datetime import date, datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "class td_sequence(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def sequence(self):\n",
    "        setup = self.data.iloc[-1]['Close'] - self.data.iloc[-1-4]['Close']\n",
    "        buy_setup = True\n",
    "        buy_counter = 1\n",
    "        sell_counter = -1\n",
    "        if setup < 0:\n",
    "            '''buy setup'''\n",
    "            buy_setup = True\n",
    "        elif setup > 0:\n",
    "            '''sell setup'''\n",
    "            buy_setup = False\n",
    "        for i in xrange(1,(len(self.data))):\n",
    "            if buy_setup:\n",
    "                buy = self.data.iloc[-1-i]['Close'] - self.data.iloc[-5-i]['Close']\n",
    "                if buy < 0:\n",
    "                    buy_counter += 1\n",
    "                    if buy_counter > 9:\n",
    "                        '''failed to reverse, reset buy counter back to 1'''\n",
    "                        buy_counter = 1 \n",
    "                    if buy_counter == 9 and ((self.data.iloc[-2-i]['Close'] - self.data.iloc[-6-i]['Close'])>0):\n",
    "                        if ((self.data.iloc[-1]['Low'] <= self.data.iloc[-3]['Low']) and (self.data.iloc[-1]['Low'] <= self.data.iloc[-4]['Low'])) or \\\n",
    "                            ((self.data.iloc[-2]['Low'] <= self.data.iloc[-3]['Low']) and (self.data.iloc[-2]['Low'] <= self.data.iloc[-4]['Low'])):\n",
    "                            buy_counter = 10\n",
    "                            return buy_counter\n",
    "                        else:\n",
    "                            return buy_counter\n",
    "                else:\n",
    "                    if (buy_counter == 8) and ((self.data.iloc[-2]['Low'] <= self.data.iloc[-3]['Low']) and (self.data.iloc[-2]['Low'] <= self.data.iloc[-4]['Low'])):\n",
    "                        buy_counter = 8.5\n",
    "                        return 8.5\n",
    "                    else:\n",
    "                        return buy_counter\n",
    "            else:\n",
    "                sell = self.data.iloc[-1-i]['Close'] - self.data.iloc[-5-i]['Close']\n",
    "                if sell > 0:\n",
    "                    sell_counter -= 1\n",
    "                    if sell_counter < -9:\n",
    "                        '''failed to reverse, reset buy counter back to -1'''\n",
    "                        sell_counter = -1 \n",
    "                    if sell_counter == -9 and ((self.data.iloc[-2-i]['Close'] - self.data.iloc[-6-i]['Close'])<0):\n",
    "                        if ((self.data.iloc[-1]['High'] > self.data.iloc[-3]['High']) and (self.data.iloc[-1]['High'] > self.data.iloc[-4]['High'])) or \\\n",
    "                            ((self.data.iloc[-2]['High'] > self.data.iloc[-3]['High']) and (self.data.iloc[-2]['High'] > self.data.iloc[-4]['High'])):\n",
    "                            sell_counter = -10\n",
    "                            return sell_counter\n",
    "                        else:\n",
    "                            return sell_counter\n",
    "                else:\n",
    "                    if sell_counter == -8 and ((self.data.iloc[-2]['High'] > self.data.iloc[-3]['High']) and (self.data.iloc[-2]['High'] > self.data.iloc[-4]['High'])):\n",
    "                        sell_counter = -8.5\n",
    "                        return -8.5\n",
    "                    else:\n",
    "                        return sell_counter\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named td_sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-04a57bd3765b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdecimal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecimal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtd_sequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTDSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m '''\n\u001b[0;32m      8\u001b[0m \u001b[0mSample\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;36m2016\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m07\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mchanges\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdate\u001b[0m \u001b[0mchanges\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named td_sequence"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datetime as dt\n",
    "import pandas.io.data as web\n",
    "from decimal import Decimal\n",
    "from td_sequence import TDSequence\n",
    "'''\n",
    "Sample data from 2016-07-25, name may changes as date changes\n",
    "Using AAPL as test\n",
    "'''\n",
    "symbol = 'AAPL'\n",
    "start = dt.datetime(2012, 1, 1)\n",
    "end = dt.date.today()\n",
    "'''DF details from before'''\n",
    "er_date = '2016-08-16'\n",
    "c = web.DataReader(symbol, 'yahoo', start, end)\n",
    "df_stock_detail = pd.read_pickle('data/2016-07-25_all_stocks_info')\n",
    "df_stock_cash_flow_quarter = pd.read_pickle('data/2016-08-19_cash_flow_quarter.pkl')\n",
    "df_stock_balance_sheet_quarter = pd.read_pickle('data/2016-08-19_balance_sheet_quarter.pkl')\n",
    "df_stock_balance_sheet_annual = pd.read_pickle('data/2016-08-19_balance_sheet_annual.pkl')\n",
    "df_stock_income_statement_quarter = pd.read_pickle('data/2016-08-19_income_statement_quarter.pkl')\n",
    "df_stock_income_statement_annual = pd.read_pickle('data/2016-08-19_income_statement_quarter.pkl')\n",
    "\n",
    "df_er = pd.read_pickle('data/rev_full_history_er_date.pkl')\n",
    "\n",
    "new_col_names = []\n",
    "for i in df_stock_detail.columns.values:\n",
    "    new_col_names.append(i.strip())\n",
    "df_stock_detail.columns = new_col_names\n",
    "\n",
    "'''sample data detail: AAPL'''\n",
    "c = web.DataReader(symbol, 'yahoo', start, end)\n",
    "earning_report = df_er[df_er['symbol'] == symbol][:2]\n",
    "earning_report['er_date'] = pd.to_datetime(earning_report['er_date'])\n",
    "earning_report['adj_er_date'] = earning_report['er_date'] if ('After' in earning_report['time'] or 'pm' in earning_report['time'])else earning_report['er_date']-dt.timedelta(days=1)\n",
    "earning_report.reset_index(drop = True, inplace = True)\n",
    "\n",
    "for i in xrange(len(earning_report)):\n",
    "    price = Decimal(c.loc[earning_report['adj_er_date'][i+1]]['Close'])\n",
    "    symbol_cash_flow_q = df_stock_cash_flow_quarter[df_stock_cash_flow_quarter['symbol'] == symbol]\n",
    "    symbol_balance_sheet_q = df_stock_balance_sheet_quarter[df_stock_balance_sheet_quarter['symbol'] == symbol]\n",
    "    symbol_income_statement_q = df_stock_income_statement_quarter[df_stock_income_statement_quarter['symbol'] == symbol]\n",
    "    symbol_cash_flow_prevQ1 = symbol_cash_flow_q.iloc[i+1]\n",
    "    symbol_cash_flow_prevQ2 = symbol_cash_flow_q.iloc[i+2]\n",
    "    symbol_balance_sheet_prevQ1 = symbol_balance_sheet_q.iloc[i+1]\n",
    "    symbol_balance_sheet_prevQ2 = symbol_balance_sheet_q.iloc[i+2]\n",
    "    symbol_income_statement_prevQ1 = symbol_income_statement_q.iloc[i+1]\n",
    "\n",
    "    '''fundamental details'''\n",
    "    preferred_stock = Decimal(0 if symbol_balance_sheet_prevQ1['Redeemable Preferred Stock, Total'] == None else symbol_balance_sheet_prevQ1['Redeemable Preferred Stock, Total']) \\\n",
    "                        + Decimal(0 if symbol_balance_sheet_prevQ1['Preferred Stock - Non Redeemable, Net'] == None else symbol_balance_sheet_prevQ1['Preferred Stock - Non Redeemable, Net']) \n",
    "    book_value = (symbol_balance_sheet_prevQ1['Total Equity'] - preferred_stock ) / symbol_balance_sheet_prevQ1['Total Common Shares Outstanding']\n",
    "    free_cash_flow = (symbol_cash_flow_prevQ1['Cash from Operating Activities'] + symbol_cash_flow_prevQ1['Capital Expenditures'])\n",
    "    total_revenue = symbol_income_statement_prevQ1['Total Revenue']\n",
    "    net_income = symbol_income_statement_prevQ1['Net Income']\n",
    "    debt = symbol_balance_sheet_prevQ1['Total Long Term Debt']\n",
    "    equity = symbol_balance_sheet_prevQ1['Total Equity']\n",
    "\n",
    "    '''value investing'''\n",
    "    price_to_book_value_q = price/book_value\n",
    "    price_to_free_cash_flow_q = price/free_cash_flow\n",
    "    price_to_earnings_q = price / symbol_income_statement_prevQ1['Diluted Normalized EPS']\n",
    "    price_to_sales_q = total_revenue / symbol_balance_sheet_prevQ1['Total Common Shares Outstanding']\n",
    "    dividends = symbol_income_statement_prevQ1['Dividends per Share - Common Stock Primary Issue']\n",
    "    long_term_debt_quarter = symbol_balance_sheet_prevQ1['Long Term Debt']\n",
    "    # long_term_debt_annual = symbol_balance_sheet_annual['Long Term Debt']\n",
    "    if max(symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'],symbol_balance_sheet_prevQ2['Total Common Shares Outstanding']) / min(symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'], symbol_balance_sheet_prevQ2['Total Common Shares Outstanding']) >= 2:\n",
    "        capital_spending_diff = symbol_cash_flow_prevQ1['Capital Expenditures']/symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'] - symbol_cash_flow_prevQ2['Capital Expenditures']/symbol_balance_sheet_prevQ1['Total Common Shares Outstanding']\n",
    "    else:\n",
    "        capital_spending_diff = symbol_cash_flow_prevQ1['Capital Expenditures']/symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'] - symbol_cash_flow_prevQ2['Capital Expenditures']/symbol_balance_sheet_prevQ2['Total Common Shares Outstanding']\n",
    "    market_cap = symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'] * price\n",
    "    return_on_total_capital =  (net_income - dividends * symbol_balance_sheet_prevQ1['Total Common Shares Outstanding']) / (debt + equity)\n",
    "    return_on_shareholders_equity = net_income/equity \n",
    "    extra_shares_outstanding = symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'] - symbol_balance_sheet_prevQ2['Total Common Shares Outstanding']\n",
    "\n",
    "    '''td_sequential'''\n",
    "    end_date = earning_report['adj_er_date'][i]\n",
    "    data = web.DataReader(symbol, 'yahoo', start, end_date)\n",
    "    td = TDSequence(data)\n",
    "    td_sequence = td.sequence()\n",
    "    '''candle_stick'''\n",
    "# Below are for futrue test data...not useful for training with lack of data\n",
    "# symbol_detail = df_stock_detail[df_stock_detail['Symbol'] == symbol]\n",
    "# symbol_quarter_idx = symbol+'_prev_1Q'\n",
    "# temp = symbol+'_' + str(dt.date.today().year)\n",
    "# temp2 = symbol+'_' + str(dt.date.today().year-1)\n",
    "# if temp in df_stock_balance_sheet_annual.index:\n",
    "#   symbol_annual_idx = temp\n",
    "# elif temp2 in df_stock_balance_sheet_annual.index:\n",
    "#     symbol_annual_idx = temp2\n",
    "# else:\n",
    "#     symbol_annual_idx = symbol+'_' + str(dt.date.today().year-2)\n",
    "\n",
    "# symbol_cash_flow_prevQ1 = df_stock_cash_flow_quarter.ix[symbol_quarter_idx]\n",
    "# symbol_balance_sheet_annual = df_stock_balance_sheet_annual.ix[symbol_annual_idx]\n",
    "# symbol_balance_sheet_prevQ1 = df_stock_balance_sheet_quarter.ix[symbol_quarter_idx]\n",
    "# symbol_balance_sheet_prevQ2 = df_stock_balance_sheet_quarter.ix[symbol+'_prev_2Q']\n",
    "# symbol_cash_flow_prevQ1 = df_stock_cash_flow_quarter.ix[symbol_quarter_idx]\n",
    "# symbol_cash_flow_prevQ2 = df_stock_cash_flow_quarter.ix[symbol+'_prev_2Q']\n",
    "\n",
    "# '''value investing'''\n",
    "# price_to_book_value = symbol_detail['Price/Book']\n",
    "# price_to_free_cash_flow_prevQ1 = c['Adj Close'][er_date]/(symbol_cash_flow_prevQ1['Cash from Operating Activities'] + symbol_cash_flow_prevQ1['Capital Expenditures'])\n",
    "# price_to_earnings = symbol_detail['P/E Ratio']\n",
    "# price_to_sales = symbol_detail['Price/Sales']\n",
    "# dividends = symbol_detail['Dividend/Share']\n",
    "# long_term_debt_quarter = symbol_balance_sheet_prevQ1['Long Term Debt']\n",
    "# long_term_debt_annual = symbol_balance_sheet_annual['Long Term Debt']\n",
    "\n",
    "# if max(symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'],symbol_balance_sheet_prevQ2['Total Common Shares Outstanding']) / min(symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'], symbol_balance_sheet_prevQ2['Total Common Shares Outstanding']) >= 2:\n",
    "#     capital_spending_diff = symbol_cash_flow_prevQ1['Capital Expenditures']/symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'] - symbol_cash_flow_prevQ2['Capital Expenditures']/symbol_balance_sheet_prevQ1['Total Common Shares Outstanding']\n",
    "# else:\n",
    "#     capital_spending_diff = symbol_cash_flow_prevQ1['Capital Expenditures']/symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'] - symbol_cash_flow_prevQ2['Capital Expenditures']/symbol_balance_sheet_prevQ2['Total Common Shares Outstanding']\n",
    "\n",
    "# market_cap = symbol_detail['Market Capitalization']\n",
    "# shares_outstanding = symbol_balance_sheet_prevQ1['Total Common Shares Outstanding'] - symbol_balance_sheet_prevQ2['Total Common Shares Outstanding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
