{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAN JOSE\n",
    "## from open street map \n",
    "\n",
    "San Jose is one of favorites area in California. San Jose has many taste food. I always to there to find interesting and new restaurant. I want to use this data to find out something interesting about this are. This is the reason I choose this area as my project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "#OSM_FILE = \"C:\\Users\\Administrator\\udacity\\data analyst\\P03\\honolulu_hawaii.osm\\honolulu_hawaii.osm\"  # Replace this with your osm file\n",
    "#OSM_FILE='C:\\Users\\Administrator\\udacity\\data analyst\\P03\\san-francisco-bay_california.osm\\san-francisco-bay_california.osm'\n",
    "OSM_FILE= 'C:\\\\Users\\\\Administrator\\\\udacity\\\\data analyst\\\\P03\\\\san-jose_california.osm\\\\san-jose_california.osm'\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osm_file='C:\\\\Users\\\\Administrator\\\\udacity\\\\data analyst\\\\P03\\\\san-jose_california.osm\\\\san-jose_california.osm'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOWER = re.compile(r'^([a-z]|_)*$')\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_element(filename):\n",
    "    \"\"\" Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = ET.iterparse(filename, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end':\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for i, elem in enumerate(get_element(filename)):\n",
    "        if elem.tag in tags:\n",
    "            tags[elem.tag] += 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "    return tags\n",
    "\n",
    "# {'member': 1322,\n",
    "#  'nd': 149685,\n",
    "#  'node': 127664,\n",
    "#  'osm': 1,\n",
    "#  'relation': 124,\n",
    "#  'tag': 68778,\n",
    "#  'way': 16971}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### showing how many entires we have in different tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 13767,\n",
       " 'nd': 1491733,\n",
       " 'node': 1276635,\n",
       " 'osm': 1,\n",
       " 'relation': 1238,\n",
       " 'tag': 685554,\n",
       " 'way': 169712}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tags(osm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def key_type(filename):\n",
    "    \"\"\" \n",
    "    Count the criteria in dictionary for the content of the tag.\n",
    "    \"\"\"\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for i, element in enumerate(get_element(filename)):\n",
    "        if element.tag == \"tag\":\n",
    "            if PROBLEMCHARS.search(element.attrib['k']):\n",
    "                keys['problemchars']+=1\n",
    "            elif LOWER.search(element.attrib['k']):\n",
    "                keys['lower'] +=1\n",
    "            elif LOWER_COLON.search(element.attrib['k']):\n",
    "                keys['lower_colon']+=1\n",
    "            else:\n",
    "                keys['other']+=1\n",
    "        \n",
    "    return keys\n",
    "\n",
    "\n",
    "#check\n",
    "#other = include capital letter\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counting for different type of tags that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 371667, 'lower_colon': 298371, 'other': 15515, 'problemchars': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_type(osm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_user(filename):\n",
    "    users = set()\n",
    "    for i,element in enumerate(get_element(filename)):\n",
    "        if \"uid\" in element.attrib:\n",
    "            users.add(element.attrib[\"uid\"])\n",
    "    return \"Total users : \" ,len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Total users : ', 1194)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user(osm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag_list(filename,item):\n",
    "    tag_type = defaultdict(set)\n",
    "    tag_type[\"regular\"]=0\n",
    "    colon=0\n",
    "    not_colon=0\n",
    "    for i, elem in enumerate(get_element(filename)):\n",
    "        if elem.tag==item:\n",
    "            for tags in elem.iter('tag'): \n",
    "                if not PROBLEMCHARS.search(tags.attrib['k']):\n",
    "                    if LOWER_COLON.search(tags.attrib['k']):\n",
    "                        colon+=1\n",
    "                        sub_attrib=tags.attrib['k'].split(':',1)\n",
    "\n",
    "                        if sub_attrib[0] in tag_type:\n",
    "                            tag_type[sub_attrib[0]]+=1\n",
    "                        else:\n",
    "                            tag_type[sub_attrib[0]]=1\n",
    "                    else: \n",
    "                        not_colon+=1\n",
    "                        tag_type[\"regular\"]+=1\n",
    "    return {'colon':colon, 'not_colon':not_colon ,'tag_type_list': tag_type}\n",
    "\n",
    "def count_tag_type(filename):\n",
    "\n",
    "    tag_item=['member','nd','node','osm','relation','way']\n",
    "\n",
    "    for item in tag_item:\n",
    "\n",
    "        a=item\n",
    "        result=tag_list(filename,item)\n",
    "        if result['colon']+result['not_colon']<=0:\n",
    "            print a,\"　:　\"\n",
    "            print a ,\" do not have tag\"\n",
    "            print \"<------------------------------------------------------------------------>\"\n",
    "        else:\n",
    "            print a\n",
    "            print \"number of items with colon: \",result['colon']\n",
    "            print \"number of items without colon: \", result['not_colon']\n",
    "            print \"list of tag type in \", a, \" :\", result['tag_type_list'].items()\n",
    "            print \"<------------------------------------------------------------------------>\"\n",
    "                 \n",
    "\n",
    "    return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counting each type in different tags\n",
    "Base on this information, we choose the type that we want to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member 　:　\n",
      "member  do not have tag\n",
      "<------------------------------------------------------------------------>\n",
      "nd 　:　\n",
      "nd  do not have tag\n",
      "<------------------------------------------------------------------------>\n",
      "node\n",
      "number of items with colon:  24145\n",
      "number of items without colon:  42175\n",
      "list of tag type in  node  : [('shop', 20), ('amenity', 37), ('telescope', 19), ('proposed', 2), ('recycling', 14), ('social_facility', 2), ('wetap', 5), ('alt_name', 4), ('gnis', 2229), ('toilets', 10), ('exit_to', 18), ('capacity', 1), ('addr', 20176), ('service', 7), ('name', 115), ('source', 254), ('railway', 7), ('fuel', 17), ('place', 1), ('is_in', 46), ('ref', 14), ('website', 7), ('caltrans', 3), ('description', 1), ('theatre', 1), ('fire_hydrant', 25), ('office', 1), ('stop', 1), ('townhall', 1), ('leisure', 1), ('outerspatial', 2), ('tiger', 47), ('flag', 14), ('regular', 42175), ('award', 1), ('socket', 11), ('old_name', 11), ('reuse', 1), ('monitoring', 5), ('payment', 37), ('population', 2), ('building', 3), ('free_flying', 3), ('census', 8), ('abandoned', 2), ('fieldcheck', 2), ('diet', 5), ('craft', 1), ('generator', 2), ('traffic_signals', 244), ('turn', 2), ('contact', 18), ('survey', 652), ('historical', 1), ('healthcare', 8), ('crossing', 9), ('tower', 7), ('disused', 4), ('locomotive', 1), ('internet_access', 3)]\n",
      "<------------------------------------------------------------------------>\n",
      "osm 　:　\n",
      "osm  do not have tag\n",
      "<------------------------------------------------------------------------>\n",
      "relation\n",
      "number of items with colon:  572\n",
      "number of items without colon:  3534\n",
      "list of tag type in  relation  : [('restriction', 2), ('golf', 2), ('social_facility', 2), ('county', 9), ('operator', 3), ('gnis', 29), ('vta', 2), ('capacity', 9), ('addr', 378), ('source', 24), ('is_in', 48), ('mrosd', 6), ('park', 7), ('tiger', 6), ('regular', 3534), ('building', 15), ('name', 6), ('contact', 1), ('public_transport', 13), ('railway', 2), ('nist', 6), ('internet_access', 2)]\n",
      "<------------------------------------------------------------------------>\n",
      "way\n",
      "number of items with colon:  273654\n",
      "number of items without colon:  341473\n",
      "list of tag type in  way  : [('shop', 1), ('nhd', 87), ('amenity', 13), ('maxspeed', 811), ('office', 1), ('recycling', 16), ('social_facility', 11), ('plant', 1), ('official_name', 2), ('alt_name', 5), ('loc_name', 1), ('lanes', 980), ('post_office', 1), ('gnis', 1313), ('vta', 403), ('note', 807), ('capacity', 377), ('addr', 43252), ('generator', 85), ('hgv', 995), ('health_specialty', 1), ('destination', 122), ('man_made', 1), ('health_facility', 1), ('cycleway', 2135), ('hov', 55), ('source', 5193), ('fuel', 15), ('aerodrome', 1), ('is_in', 156), ('website', 3), ('caltrans', 9), ('junction', 8), ('tower', 1), ('theatre', 1), ('src', 45), ('mrosd', 83), ('park', 2), ('diet', 2), ('tiger', 209172), ('regular', 341473), ('toilets', 4), ('old_name', 4), ('payment', 7), ('change', 1), ('name', 87), ('building', 563), ('gns', 4), ('placement', 97), ('abandoned', 4), ('atm', 2), ('ramp', 5), ('traffic_signals', 13), ('turn', 6565), ('contact', 28), ('survey', 13), ('roof', 11), ('healthcare', 4), ('mtb', 12), ('railway', 58), ('internet_access', 4)]\n",
      "<------------------------------------------------------------------------>\n"
     ]
    }
   ],
   "source": [
    "count_tag_type(osm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we are going to check street name and postcode\n",
    "we aduit the data that we choose, and display the items that is not what we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "street_type_re=re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "zipcode_re =re.compile(r\"\\d{5}?\")\n",
    "no_number_re=re.compile(r'(?=(\\w*\\s?\\w+\\s\\w+)\\s+[Ste|Suite|PM])', re.IGNORECASE)\n",
    "street_name_only_re= re.compile(r\"\\A[\\w\\s]*\", re.IGNORECASE)\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Way\", \"Terrace\", \"Circle\", \"Real\", \"Expressway\", \"Highway\"]\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k']==\"addr:street\")\n",
    "\n",
    "def aduit_street_type(not_expected,street_name):\n",
    "    m=street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type=m.group()\n",
    "        if street_type not in expected:\n",
    "            not_expected[street_type].add(street_name)\n",
    "    \n",
    "def is_postcode(elem):\n",
    "    return (elem.attrib['k']==\"addr:postcode\")\n",
    "\n",
    "def aduit_zipcode(not_expected,postcode):\n",
    "    if len(postcode)>5:\n",
    "        not_expected = postcode\n",
    "        return not_expected\n",
    "    \n",
    "def aduit_tag(filename,loc,test,aduit):\n",
    "    not_expected=defaultdict(set)\n",
    "    for i, elem in enumerate(get_element(filename)):\n",
    "        if elem.tag==loc:\n",
    "            for tag in elem.iter('tag'):\n",
    "                if test(tag):\n",
    "                    aduit(not_expected, tag.attrib['v'])\n",
    "                    \n",
    "    pprint.pprint(dict(not_expected))   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update data\n",
    "After we aduit the data, we check the display and change the data to what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Dr\" : \"Drive\",\n",
    "            \"sq\": \"Square\",\n",
    "            \"Sq\": \"Square\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Boulvevard\": \"Boulevard\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Express Way\": \"Expressway\",\n",
    "            \"ave\": \"Avenue\",\n",
    "            \"street\": \"Street\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"Pkwy\": \"Parkway\"\n",
    "            }\n",
    "audited_file = \"osm_file_audit.osm\"\n",
    "def update_name(name, mapping):\n",
    "\n",
    "\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "\n",
    "        if street_type not in expected:\n",
    "            if street_type in mapping:\n",
    "                name = re.sub(street_type_re, mapping[street_type], name)\n",
    "                \n",
    "    return name\n",
    "\n",
    "\n",
    "#'CA': set(['Zanker Rd., San Jose, CA', 'Zanker Road, San Jose, CA'])\n",
    "def street_only(name):\n",
    "    s = street_type_re.search(name)\n",
    "    if s:\n",
    "        street_type = s.group()\n",
    "        if street_type == \"CA\":\n",
    "            name =street_name_only_re.match(name).group(0)\n",
    "            return name\n",
    "        if not street_type.isalpha():\n",
    "            if \"Ste\" or \"Suite\"  in name:\n",
    "                name=no_number_re.findall(name)[0]\n",
    "                return name\n",
    "    return name        \n",
    "\n",
    "def update_postcode(postcode):\n",
    "    postcode=re.sub(\"\\D*\", \"\",postcode)\n",
    "    if len(postcode)>5:\n",
    "\n",
    "        postcode=zipcode_re.search(postcode).group(0)\n",
    "        return postcode\n",
    "    return postcode\n",
    "\n",
    "\n",
    "\n",
    "def update_tag(filename):\n",
    "\n",
    "    \n",
    "    \n",
    "    with open(audited_file, 'wb') as output:\n",
    "        output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        output.write('<osm>\\n  ')    \n",
    "\n",
    "\n",
    "        for i, elem in enumerate(get_element(filename)):\n",
    "            if  elem.tag==\"way\" or elem.tag==\"node\" or elem.tag==\"relation\":\n",
    "                for tag in elem.iter('tag'):\n",
    "                    if is_street_name(tag):\n",
    "                        tag.attrib['v']= street_only(tag.attrib['v'])\n",
    "                        tag.attrib['v'] = update_name(tag.attrib['v'], mapping)\n",
    "                    \n",
    "                    if is_postcode(tag):\n",
    "                        tag.attrib['v'] = update_postcode(tag.attrib['v'])\n",
    "                        \n",
    "            output.write(ET.tostring(elem, encoding='utf-8'))\n",
    "\n",
    "        output.write('</osm>')                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def update_postcode(postcode):\n",
    "    postcode=re.sub(\"\\D*\", \"\",postcode)\n",
    "    if len(postcode)>5:\n",
    "        \n",
    "        print postcode\n",
    "\n",
    "        postcode=zipcode_re.search(postcode).group(0)\n",
    "        return postcode\n",
    "    print type(postcode)\n",
    "    return postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when I try to clean up the data for postcode , I find out some user put city name in postcode field, so I have to check for postcode and replace the city name with a empty string. Another problem I find in postcode field is that some postcode is longer than 5 digits which is zipcode + indicate. I do not want the indicate, so I took the first five digits of those postcode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when I clean up the data for street type, I do not want abbreviation, so I update all the abbreviation. I also update the street type to only begin with a capital letter. At the same time, I fixed the wrong spelling. \n",
    "\n",
    "### after clean up the street type, I notice some street name is including suit number or the whole address. I took the street name out from the data and then update it as the street name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'680': set(['Ala 680']),\n",
      " 'Alameda': set(['The Alameda']),\n",
      " 'Barcelona': set(['Calle de Barcelona']),\n",
      " 'Bascom': set(['S. Bascom']),\n",
      " 'Cir': set(['Celadon Cir']),\n",
      " 'East': set(['Park Circle East', 'Vanderbilt Court East']),\n",
      " 'Esquela': set(['Camina Esquela']),\n",
      " 'Ln': set(['Gaundabert Ln']),\n",
      " 'Loop': set(['Infinite Loop', 'Village View Loop']),\n",
      " 'Luna': set(['Calle de Luna']),\n",
      " 'Madrid': set(['Corte de Madrid']),\n",
      " 'Marino': set(['Via San Marino']),\n",
      " 'Napoli': set(['Via Napoli']),\n",
      " 'Palamos': set(['Via Palamos']),\n",
      " 'Paviso': set(['Via Paviso']),\n",
      " 'Plaza': set(['Rivermark Plaza']),\n",
      " 'Portofino': set(['Via Portofino']),\n",
      " 'Presada': set(['Paseo Presada']),\n",
      " 'Seville': set(['Corte de Seville']),\n",
      " 'Sorrento': set(['Via Sorrento']),\n",
      " 'Volante': set(['Via Volante']),\n",
      " 'West': set(['Park Circle West', 'Vanderbilt Court West']),\n",
      " 'robles': set(['rio robles'])}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "aduit_tag(audited_file,'way',is_street_name,aduit_street_type)\n",
    "aduit_tag(audited_file,'relation',is_postcode ,aduit_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'17': set(['Hwy 17']),\n",
      " 'Alameda': set(['The Alameda']),\n",
      " 'Bellomy': set(['Bellomy']),\n",
      " 'East': set(['Park Circle East', 'Rio Robles East']),\n",
      " 'Flores': set(['Terreno De Flores']),\n",
      " 'Franklin': set(['Franklin']),\n",
      " 'Hamilton': set(['Mount Hamilton']),\n",
      " 'Hill': set(['Blossom Hill']),\n",
      " 'Julian': set(['West Julian']),\n",
      " 'Loop': set(['Infinite Loop']),\n",
      " 'Oro': set(['Via del Oro']),\n",
      " 'Pkwy': set(['Great America Pkwy']),\n",
      " 'Plaza': set(['Portal Plaza', 'Rivermark Plaza']),\n",
      " 'Presada': set(['Paseo Presada']),\n",
      " 'Row': set(['Santana Row']),\n",
      " 'Sorrento': set(['Via Sorrento']),\n",
      " 'Volante': set(['Via Volante']),\n",
      " 'Walk': set(['Paseo de San Antonio Walk']),\n",
      " 'West': set(['Park Circle West']),\n",
      " 'Winchester': set(['Winchester'])}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "aduit_tag(audited_file,'node',is_street_name,aduit_street_type)\n",
    "aduit_tag(audited_file,'node',is_postcode ,aduit_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Paviso': set(['Via Paviso'])}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "aduit_tag(audited_file,'relation',is_street_name,aduit_street_type)\n",
    "aduit_tag(audited_file,'relation',is_postcode ,aduit_zipcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping data \n",
    "I group the data in 5 different field for later use in SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NODE_FIELDS = ['id', 'user', 'uid', 'version','lat','lon', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "def shape_element(element):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  \n",
    "\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for a in element.attrib:\n",
    "            if a in NODE_FIELDS:\n",
    "                if a not in node_attribs:\n",
    "\n",
    "                    node_attribs[a]=element.attrib[a]\n",
    "        \n",
    "     \n",
    "\n",
    "        for tag in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tag.attrib['k']):\n",
    "                \n",
    "                    \n",
    "                each_tag={\n",
    "                            \"id\":None,\n",
    "                            \"key\":None,\n",
    "                            \"value\":None,\n",
    "                            \"type\":None\n",
    "                }\n",
    "                each_tag['id']=node_attribs['id']\n",
    "                each_tag['value']=tag.attrib['v']\n",
    "                if LOWER_COLON.search(tag.attrib['k']):\n",
    "                    \n",
    "                    sub_attrib=tag.attrib['k'].split(':',1)\n",
    "                        \n",
    "                    each_tag['key']=sub_attrib[1]\n",
    "\n",
    "                    each_tag['type']=sub_attrib[0]\n",
    "                else:\n",
    "                    each_tag['key']=tag.attrib['k']\n",
    "                    each_tag['type']='regular'\n",
    "                    \n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "                tags.append(each_tag)\n",
    "\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        for a in element.attrib:\n",
    "            if a in WAY_FIELDS:\n",
    "                if a not in way_attribs:\n",
    "\n",
    "                    way_attribs[a]=element.attrib[a]\n",
    "        count=0\n",
    "        for nd in element.iter('nd'):\n",
    "            \n",
    "            each_way_node={}\n",
    "            \n",
    "            \n",
    "            each_way_node['position']=count\n",
    "            count+=1\n",
    "            each_way_node['node_id']=nd.attrib['ref']\n",
    "            each_way_node['id']=way_attribs['id']\n",
    "            way_nodes.append(each_way_node)\n",
    "\n",
    "        \n",
    "        for tag in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tag.attrib['k']):\n",
    "                \n",
    "                    \n",
    "                each_tag={}\n",
    "                each_tag['id']=way_attribs['id']\n",
    "                each_tag['value']=tag.attrib['v']\n",
    "                if LOWER_COLON.search(tag.attrib['k']):\n",
    "                    \n",
    "                    sub_attrib=tag.attrib['k'].split(':',1)\n",
    "                        \n",
    "                    each_tag['key']=sub_attrib[1]\n",
    "\n",
    "                    each_tag['type']=sub_attrib[0]\n",
    "                else:\n",
    "                    each_tag['key']=tag.attrib['k']\n",
    "                    each_tag['type']='regular'\n",
    "                    \n",
    "        \n",
    "                tags.append(each_tag)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write data into csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_map(file_in):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "    data = []\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file,\\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "        \n",
    "        \n",
    "        for element in get_element(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'node': {'changeset': '11686320',\n",
      "           'id': '25457954',\n",
      "           'lat': '37.1582245',\n",
      "           'lon': '-121.6574737',\n",
      "           'timestamp': '2012-05-24T03:24:59Z',\n",
      "           'uid': '14293',\n",
      "           'user': 'KindredCoda',\n",
      "           'version': '10'},\n",
      "  'node_tags': []},\n",
      " {'node': {'changeset': '11721513',\n",
      "           'id': '25457955',\n",
      "           'lat': '37.1648535',\n",
      "           'lon': '-121.6653013',\n",
      "           'timestamp': '2012-05-27T23:43:57Z',\n",
      "           'uid': '14293',\n",
      "           'user': 'KindredCoda',\n",
      "           'version': '10'},\n",
      "  'node_tags': []},\n",
      " {'node': {'changeset': '11721513',\n",
      "           'id': '25457957',\n",
      "           'lat': '37.186653',\n",
      "           'lon': '-121.6871419',\n",
      "           'timestamp': '2012-05-27T23:43:57Z',\n",
      "           'uid': '14293',\n",
      "           'user': 'KindredCoda',\n",
      "           'version': '15'},\n",
      "  'node_tags': []},\n",
      " {'node': {'changeset': '11721596',\n",
      "           'id': '25457960',\n",
      "           'lat': '37.2273856',\n",
      "           'lon': '-121.7433767',\n",
      "           'timestamp': '2012-05-28T00:02:33Z',\n",
      "           'uid': '14293',\n",
      "           'user': 'KindredCoda',\n",
      "           'version': '15'},\n",
      "  'node_tags': []},\n",
      " {'node': {'changeset': '755914',\n",
      "           'id': '25457961',\n",
      "           'lat': '37.2578667',\n",
      "           'lon': '-121.7968539',\n",
      "           'timestamp': '2008-11-16T05:13:41Z',\n",
      "           'uid': '54598',\n",
      "           'user': 'adbrown',\n",
      "           'version': '22'},\n",
      "  'node_tags': []},\n",
      " {'node': {'changeset': '755914',\n",
      "           'id': '25457963',\n",
      "           'lat': '37.2606169',\n",
      "           'lon': '-121.7992469',\n",
      "           'timestamp': '2008-11-16T05:13:41Z',\n",
      "           'uid': '54598',\n",
      "           'user': 'adbrown',\n",
      "           'version': '20'},\n",
      "  'node_tags': []}]\n"
     ]
    }
   ],
   "source": [
    "data = process_map(audited_file)\n",
    "pprint.pprint(data[0:6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
